{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EE-411 Homework 3 : Neural Networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Name : Palmisano*\n",
    "\n",
    "*First Name : Fabio Nicola Edouard*\n",
    "\n",
    "*Sciper = 296708*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 1 : Backpropagation with logistic loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\",category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1) Function Predict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X : Array of shape B X D, W = {w_1 : D x K, w_2 : K x 1}\n",
    "\n",
    "#Directly taken from the TP9\n",
    "\n",
    "def np_sigmoid(t):\n",
    "    \"\"\"apply sigmoid function on t.\"\"\"\n",
    "    return 1.0 / (1 + np.exp(-t))\n",
    "\n",
    "def predict(X,W) :\n",
    "    x_0 = X\n",
    "    z_1 = np.dot(X,W[\"w_1\"])\n",
    "    X_1 = np_sigmoid(z_1)\n",
    "    z_2 = np.dot(X_1,W[\"w_2\"])\n",
    "    y_hat = np_sigmoid(z_2)\n",
    "    return (z_1, z_2, y_hat)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 5)\n",
      "(100, 1)\n",
      "(100, 1)\n"
     ]
    }
   ],
   "source": [
    "D = 7\n",
    "K = 5\n",
    "B = 100\n",
    "\n",
    "X = np.random.rand(B, D)\n",
    "W = {\"w_1\": np.random.rand(D, K), \"w_2\": np.random.rand(K, 1)}\n",
    "\n",
    "z_1, z_2, y_hat = predict(X, W)\n",
    "print(z_1.shape)\n",
    "print(z_2.shape)\n",
    "print(y_hat.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2) Function Logistic Loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def logistic_loss(y, y_hat):\n",
    "    \"\"\"Compute the average logistic loss of a batch.\"\"\"\n",
    "    avg_log_loss = np.mean(-y * np.log(y_hat) - (1 - y) * np.log(1 - y_hat))\n",
    "    return avg_log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = nan\n"
     ]
    }
   ],
   "source": [
    "y_hat = np.zeros((B, 1))\n",
    "y = np.zeros((B, 1))\n",
    "loss = logistic_loss(y, y_hat)\n",
    "print(f\"loss = {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It equal to zero because we can't calcul the logarithm of zero so that give us the Nan result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3) Function  stable logistic loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stable_loss = 0.000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def stable_logistic_loss(y, z2):\n",
    "    \"\"\"Compute the average stable logistic loss of a batch.\"\"\"\n",
    "    avg_loss = np.mean(np.logaddexp(0, -z2) + (1-y) * z2)\n",
    "    return avg_loss\n",
    "\n",
    "z2 = -10e10 * np.ones(B)\n",
    "y = np.zeros((B, 1))\n",
    "stable_loss = stable_logistic_loss(y, z2)\n",
    "print(f\"stable_loss = {stable_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have a stable implementation of our logistic loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4) Derive Analytically (Notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5) Implement Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(X, y, W):\n",
    "    \"\"\"Compute the gradient of the average loss with respect to all the weights.\"\"\"\n",
    "    B = X.shape[0]\n",
    "    z_1, z_2, y_hat = predict(X, W)\n",
    "    dL_dy_hat = (1/B) * (y_hat - y)\n",
    "    dL_dz_2 = dL_dy_hat * y_hat * (1 - y_hat)\n",
    "    dL_dX_1 = np.dot(dL_dz_2, W[\"w_2\"].T)\n",
    "    dL_dz_1 = dL_dX_1 * z_1 * (1 - z_1)\n",
    "    dL_dW_2 = np.dot(z_1.T, dL_dz_2)\n",
    "    dL_dW_1 = np.dot(X.T, dL_dz_1)\n",
    "    gradient = {\"dL_dW_1\": dL_dW_1, \"dL_dW_2\": dL_dW_2}\n",
    "    return gradient\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scientific",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
