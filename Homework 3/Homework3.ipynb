{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EE-411 Homework 3 : Neural Networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Name : Palmisano*\n",
    "\n",
    "*First Name : Fabio Nicola Edouard*\n",
    "\n",
    "*Sciper = 296708*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 1 : Backpropagation with logistic loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\",category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1) Function Predict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X : Array of shape B X D, W = {w_1 : D x K, w_2 : K x 1}\n",
    "\n",
    "#Directly taken from the TP9\n",
    "\n",
    "def np_sigmoid(t):\n",
    "    \"\"\"apply sigmoid function on t.\"\"\"\n",
    "    return 1.0 / (1 + np.exp(-t))\n",
    "\n",
    "def predict(X,W) :\n",
    "    x_0 = X\n",
    "    z_1 = np.dot(X,W[\"w_1\"])\n",
    "    X_1 = np_sigmoid(z_1)\n",
    "    z_2 = np.dot(X_1,W[\"w_2\"])\n",
    "    y_hat = np_sigmoid(z_2)\n",
    "    return (z_1, z_2, y_hat)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 5)\n",
      "(100, 1)\n",
      "(100, 1)\n"
     ]
    }
   ],
   "source": [
    "D = 7\n",
    "K = 5\n",
    "B = 100\n",
    "\n",
    "X = np.random.rand(B, D)\n",
    "W = {\"w_1\": np.random.rand(D, K), \"w_2\": np.random.rand(K, 1)}\n",
    "\n",
    "z_1, z_2, y_hat = predict(X, W)\n",
    "print(z_1.shape)\n",
    "print(z_2.shape)\n",
    "print(y_hat.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2) Function Logistic Loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def logistic_loss(y, y_hat):\n",
    "    \"\"\"Compute the average logistic loss of a batch.\"\"\"\n",
    "    avg_log_loss = np.mean(-y * np.log(y_hat) - (1 - y) * np.log(1 - y_hat))\n",
    "    return avg_log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = nan\n"
     ]
    }
   ],
   "source": [
    "y_hat = np.zeros((B, 1))\n",
    "y = np.zeros((B, 1))\n",
    "loss = logistic_loss(y, y_hat)\n",
    "print(f\"loss = {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It equal to zero because we can't calcul the logarithm of zero so that give us the Nan result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3) Function  stable logistic loss:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we start form the equation of the loss : \n",
    "$\\begin{equation}\n",
    "\\mathcal{L} (y, \\hat{y}) = -y \\log(\\hat{y}) - (1 - y) \\log(1 - \\hat{y}) \\ \\ \\ with \\ \\ \\ \\hat{y} = \\frac{1}{1+e^{-z_2}}\n",
    "\\end{equation}$\n",
    "The we replace $\\hat{y}$ in this equation by his value and we obtain\n",
    "$\\begin{equation}\n",
    "y \\log(1+e^{-z_2}) - (1 - y) \\log(1 - \\frac{1}{1+e^{-z_2}}) = y \\log(1+e^{-z_2}) - (1 - y) \\log(\\frac{e^{-z_2}}{1+e^{-z_2}}) =  y \\log(1+e^{-z_2}) + (1 - y)*z_2 + (1-y)\\log(1+e^{-z_2})\n",
    "\\end{equation}$\n",
    "Looking at the Documentation we know that $ logaddexp (a,b)= \\log(e^{a} + e^{b})$ we have :\n",
    "$\\begin{equation}\n",
    "\\log(1+e^{-z_2}) + (1 - y)*z_2  = logaddexp(0,-z_2) + (1-y)*z_2\n",
    "\\end{equation}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stable_loss = 0.000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def stable_logistic_loss(y, z2):\n",
    "    \"\"\"Compute the average stable logistic loss of a batch.\"\"\"\n",
    "    avg_loss = np.mean(np.logaddexp(0, -z2) + (1-y) * z2)\n",
    "    return avg_loss\n",
    "\n",
    "z2 = -10e10 * np.ones(B)\n",
    "y = np.zeros((B, 1))\n",
    "stable_loss = stable_logistic_loss(y, z2)\n",
    "print(f\"stable_loss = {stable_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have a stable implementation of our logistic loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4) Derive Analytically (Notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{equation}\n",
    "\\dfrac{\\partial \\mathcal{L}}{\\partial w^{(1)}_{i,j}}=\\dfrac{\\partial \\mathcal{L}}{\\partial z^{(2)}_{i}} \\cdot \\dfrac{\\partial z^{(2)}_{i}}{\\partial z^{(1)}_{i}} \\cdot \\dfrac{\\partial z^{(1)}_{i}}{\\partial w^{(1)}_{i,j}} \\ \\ \\ and \\ \\ \\          \\dfrac{\\partial \\mathcal{L}}{\\partial w^{(2)}_{i}}=\\dfrac{\\partial \\mathcal{L}}{\\partial z^{(1)}_{i}} \\cdot \\dfrac{\\partial z^{(2)}_{i}}{\\partial w^{(2)}_{i}}\n",
    "\\end{equation}$\n",
    "Developping what we obtain above we add : \n",
    "$\\begin{equation}\n",
    "\\dfrac{\\partial \\mathcal{L}}{\\partial z^{(2)}_{i}} = (1 - y) - \\frac{e^{-z_i^{2}}}{1+e^{-z_i^{2}}} = (1-y) - ( 1 - \\sigma(z_i^{2})) = (1-y) - ( 1 - \\hat{y}) =  \\hat{y} - y \n",
    "\\end{equation}$\n",
    "\n",
    "$\\begin{equation}\n",
    "\\dfrac{\\partial z^{(2)}_{i}}{\\partial z^{(1)}_{i}} = \\dfrac{\\partial(w^{(2)} x^{(1)})}{\\partial z^{(1)}_{i}} = w^{(2)} \\sigma(z_i^{(1)}) (1 - \\sigma(z_i^{(1)}))\n",
    "\\end{equation}$\n",
    "\n",
    "\n",
    "$\\begin{equation}\n",
    "\\dfrac{\\partial z^{(1)}_{i}}{\\partial w^{(1)}_{i,j}} = x^{(0)} \\ \\ \\ and \\ \\ \\ \\dfrac{\\partial z^{(2)}_{i}}{\\partial w^{(2)}_{i}} = x^{(1)} = \\sigma(z_i^{(1)})\n",
    "\\end{equation}$\n",
    "\n",
    "And we finally obtain this result : \n",
    "\n",
    "$\\begin{equation}\n",
    "\\dfrac{\\partial \\mathcal{L}}{\\partial w^{(1)}_{i,j}} = (\\hat{y} - y) \\cdot w^{(2)} \\cdot \\sigma(z_i^{(1)}) (1 - \\sigma(z_i^{(1)})) \\cdot  x^{(0)}  \\ \\ \\ and \\ \\ \\ \\dfrac{\\partial \\mathcal{L}}{\\partial w^{(2)}_{i}}= (\\hat{y} - y)\\cdot \\sigma(z_i^{(1)})\n",
    "\\end{equation}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5) Implement Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(X, y, W):\n",
    "    \"\"\"Compute the gradient of the average loss with respect to all the weights.\"\"\"\n",
    "    z_1, z_2, y_hat = predict(X, W)\n",
    "    print(y_hat.shape)\n",
    "    print(z_1.shape)\n",
    "    print(z_2.shape)\n",
    "    dLdz2 = y_hat - y\n",
    "    dLdw2 = np.dot(np_sigmoid(z_1).T, dLdz2)\n",
    "    dz2dz1 = W[\"w_2\"].T * np_sigmoid(z_1) * (1 - np_sigmoid(z_1))\n",
    "    dLdw1 = np.dot(X.T, dLdz2 * dz2dz1)\n",
    "    return (dLdw1, dLdw2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1)\n",
      "(100, 5)\n",
      "(100, 1)\n",
      "(7, 5)\n",
      "(5, 1)\n"
     ]
    }
   ],
   "source": [
    "D = 7\n",
    "K = 5\n",
    "B = 100\n",
    "\n",
    "X = np.random.rand(B, D)-0.5\n",
    "W = {\n",
    "    \"w_1\": np.random.rand(D, K)-0.5,\n",
    "    \"w_2\": np.random.rand(K, 1)-0.5\n",
    "}\n",
    "\n",
    "g_w1, g_w2 = gradient(X, y, W)\n",
    "print(g_w1.shape)\n",
    "print(g_w2.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 2 : Classifying KMNIST using neural networks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we load all the necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1024\n",
    "TEST_BATCH_SIZE = 2048\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find out which device is available\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Compose([T.ToTensor()])\n",
    "\n",
    "# load the train dataset\n",
    "train_dataset = torchvision.datasets.KMNIST(\n",
    "    root='./data/', \n",
    "    train=True, \n",
    "    download=True,\n",
    "    transform=transform)\n",
    "\n",
    "# load the test dataset\n",
    "test_dataset = torchvision.datasets.KMNIST(\n",
    "    root='./data/', \n",
    "    train=False, \n",
    "    download=True,\n",
    "    transform=transform)\n",
    "\n",
    "# Create the validation dataset from the train dataset\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [50000, 10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the dataloader for the traininig dataset. \n",
    "# Here we shuffle the data to promote stochasticity.\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True, \n",
    "    num_workers=2)\n",
    "\n",
    "\n",
    "# Construct the dataloader for the testing dataset.\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False, \n",
    "    num_workers=2)\n",
    "\n",
    "validation_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=val_dataset, \n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    "    shuffle=False, \n",
    "    num_workers=2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first 10 images of the train dataset. Hint: use next(), iter()\n",
    "images = next(iter(train_dataloader))[0][:10]\n",
    "grid = torchvision.utils.make_grid(images, nrow=5, padding=10)\n",
    "\n",
    "def show(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')\n",
    "\n",
    "show(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANSWER THE Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2) MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def device_type():\n",
    "    if torch.cuda.is_available():\n",
    "        return 'cuda'\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return 'mps'\n",
    "    else:\n",
    "        return 'cpu'\n",
    "DEVICE = torch.device(device_type())\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 100)\n",
    "        self.fc2 = nn.Linear(100, 100)\n",
    "        self.fc3 = nn.Linear(100, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x.view(-1, 784)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        output = self.fc3(x)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "models = []\n",
    "def train_epoch(\n",
    "    model: nn.Module, \n",
    "    train_dataloader: DataLoader, \n",
    "    optimizer: torch.optim.Optimizer, \n",
    "    device: torch.device\n",
    "    ):\n",
    "    '''\n",
    "    This function implements the core components of any Neural Network training regiment.\n",
    "    In our stochastic setting our code follows a very specific \"path\". First, we load the batch\n",
    "    a single batch and zero the optimizer. Then we perform the forward pass, compute the gradients and perform the backward pass. And ...repeat!\n",
    "    '''\n",
    "    models.append(model)\n",
    "    running_loss = 0.0\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_dataloader):\n",
    "        # move data and target to device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # do the forward pass\n",
    "        output = model(data)\n",
    "\n",
    "        # compute the loss\n",
    "        loss = F.cross_entropy(output, target)\n",
    "\n",
    "        # compute the gradients\n",
    "        loss.backward()\n",
    "\n",
    "\n",
    "        # perform the gradient step\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    return running_loss / len(train_dataloader.dataset)\n",
    "\n",
    "\n",
    "def fit(\n",
    "    model: nn.Module, \n",
    "    train_dataloader: DataLoader, \n",
    "    optimizer: torch.optim.Optimizer, \n",
    "    epochs: int, \n",
    "    device: torch.device):\n",
    "    '''\n",
    "    the fit method simply calls the train_epoch() method for a \n",
    "    specified number of epochs.\n",
    "    '''\n",
    "\n",
    "    # keep track of the losses in order to visualize them later\n",
    "    # Train for numerous epochs:\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = train_epoch(\n",
    "            model=model, \n",
    "            train_dataloader=train_dataloader, \n",
    "            optimizer=optimizer, \n",
    "            device=device\n",
    "        )\n",
    "        #print(f\"Epoch {epoch}: Loss={running_loss}\")\n",
    "        losses.append(running_loss)\n",
    "    print('Finished Training')\n",
    "    return losses\n",
    "\n",
    "def predict(model: nn.Module, test_dataloader: DataLoader, device: torch.device):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_dataloader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = F.cross_entropy(output, target)\n",
    "            test_loss += loss.item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "\n",
    "    test_loss /= len(test_dataloader.dataset)\n",
    "    accuracy = 100. * correct / len(test_dataloader.dataset)\n",
    "    accuracies.append(accuracy)\n",
    "    print(f'Test set: Avg. loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_dataloader.dataset)} ({accuracy:.0f}%)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "#Model with Optimizer : SGD and Learning Rate : 0.01\n",
    "modelSGD_001 = Net()\n",
    "optimizerSGD_001 = optim.SGD(modelSGD_001.parameters(), lr=0.01)\n",
    "print('SGD with LR = 0.01')\n",
    "lossesSGD_001 = fit(modelSGD_001, train_dataloader, optimizerSGD_001, EPOCHS, device = DEVICE)\n",
    "predict(modelSGD_001, test_dataloader, device = DEVICE)\n",
    "plt.plot(lossesSGD_001, label = \"SGD with LR = 0.01\")\n",
    "\n",
    "#Model with Optimizer : SGD with momentum and Learning Rate : 0.01, momentum : 0.9, nestrov : True\n",
    "modelSGD_001_momentum = Net()\n",
    "optimizerSGD_001_momentum = optim.SGD(modelSGD_001_momentum.parameters(), lr=0.01, momentum=0.9, nesterov=True)\n",
    "print('SGD with LR = 0.01, momentum = 0.9, nestrov = True')\n",
    "lossesSGD_001_momentum = fit(modelSGD_001_momentum, train_dataloader, optimizerSGD_001_momentum, EPOCHS, device = DEVICE)\n",
    "predict(modelSGD_001_momentum, test_dataloader, device = DEVICE)\n",
    "plt.plot(lossesSGD_001_momentum, label = \"SGD with LR = 0.01, momentum = 0.9, nestrov = True\")\n",
    "\n",
    "#Model with Adam Optimizer and Learning Rate : 0.01\n",
    "modelAdam_001 = Net()\n",
    "optimizerAdam_001 = optim.Adam(modelAdam_001.parameters(), lr=0.01)\n",
    "print('Adam with LR = 0.01')\n",
    "lossesAdam_001 = fit(modelAdam_001, train_dataloader, optimizerAdam_001, EPOCHS, device = DEVICE)\n",
    "predict(modelAdam_001, test_dataloader, device = DEVICE)\n",
    "plt.plot(lossesAdam_001, label = \"Adam with LR = 0.01\")\n",
    "\n",
    "#Model with Adam Optimizer and Learning Rate : 1\n",
    "modelAdam_1 = Net()\n",
    "optimizerAdam_1 = optim.Adam(modelAdam_1.parameters(), lr=1)\n",
    "print('Adam with LR = 1')\n",
    "lossesAdam_1 = fit(modelAdam_1, train_dataloader, optimizerAdam_1, EPOCHS, device = DEVICE)\n",
    "predict(modelAdam_1, test_dataloader, device = DEVICE)\n",
    "plt.plot(lossesAdam_1,  label = \"Adam with LR = 1\")\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"Loss progression across epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Validation Accuracy with SGD with LR = 0.01')\n",
    "predict(modelSGD_001, validation_dataloader, device = DEVICE)\n",
    "models.append('modelSGD_001')\n",
    "print('----------------------------------------------------------------------')\n",
    "print('Validation Accuracy with SGD with LR = 0.01, momentum = 0.9, nestrov = True')\n",
    "predict(modelSGD_001_momentum, validation_dataloader, device = DEVICE)\n",
    "models.append('modelSGD_001_momentum')\n",
    "print('----------------------------------------------------------------------')\n",
    "print('Validation Accuracy with Adam with LR = 0.01')\n",
    "predict(modelAdam_001, validation_dataloader, device = DEVICE)\n",
    "models.append('modelAdam_001')\n",
    "print('----------------------------------------------------------------------')\n",
    "print('Validation Accuracy with Adam with LR = 1')\n",
    "models.append('modelAdam_1')\n",
    "predict(modelAdam_1, validation_dataloader, device = DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can directly see that the model with SGD with momentum have a higher Accuracy score. However we can do better taking the optimizer Adam but with a learning rate not too big. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3) Same but with CNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module): \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 1, out_channels = 16, kernel_size = 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2),\n",
    "            nn.Conv2d(16, 32, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.fc = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "#Model with Optimizer : SGD and Learning Rate : 0.01\n",
    "modelSGD_001_CNN = CNN()\n",
    "optimizerSGD_001_CNN = optim.SGD(modelSGD_001_CNN.parameters(), lr=0.01)\n",
    "print('SGD in CNN with LR = 0.01')\n",
    "lossesSGD_001_CNN = fit(modelSGD_001_CNN, train_dataloader, optimizerSGD_001_CNN, EPOCHS, device = DEVICE)\n",
    "predict(modelSGD_001_CNN, test_dataloader, device = DEVICE)\n",
    "plt.plot(lossesSGD_001_CNN, label = \"SGD in CNN with LR = 0.01\")\n",
    "\n",
    "#Model with Optimizer : SGD with momentum and Learning Rate : 0.01, momentum : 0.9, nestrov : True\n",
    "modelSGD_001_momentum_CNN = CNN()\n",
    "optimizerSGD_001_momentum_CNN = optim.SGD(modelSGD_001_momentum_CNN.parameters(), lr=0.01, momentum=0.9, nesterov=True)\n",
    "print('SGD in CNN with LR = 0.01, momentum = 0.9, nestrov = True')\n",
    "lossesSGD_001_momentum_CNN = fit(modelSGD_001_momentum_CNN, train_dataloader, optimizerSGD_001_momentum_CNN, EPOCHS, device = DEVICE)\n",
    "predict(modelSGD_001_momentum_CNN, test_dataloader, device = DEVICE)\n",
    "plt.plot(lossesSGD_001_momentum_CNN, label = \"SGD in CNN with LR = 0.01, momentum = 0.9, nestrov = True\")\n",
    "\n",
    "#Model with Adam Optimizer and Learning Rate : 0.01\n",
    "modelAdam_001_CNN = CNN()\n",
    "optimizerAdam_001_CNN = optim.Adam(modelAdam_001_CNN.parameters(), lr=0.01)\n",
    "print('Adam in CNN with LR = 0.01')\n",
    "lossesAdam_001_CNN = fit(modelAdam_001_CNN, train_dataloader, optimizerAdam_001_CNN, EPOCHS, device = DEVICE)\n",
    "predict(modelAdam_001_CNN, test_dataloader, device = DEVICE)\n",
    "plt.plot(lossesAdam_001_CNN, label = \"Adam in CNN with LR = 0.01\")\n",
    "\n",
    "#Model with Adam Optimizer and Learning Rate : 1\n",
    "modelAdam_1_CNN = CNN()\n",
    "optimizerAdam_1_CNN = optim.Adam(modelAdam_1_CNN.parameters(), lr=1)\n",
    "print('Adam in CNN with LR = 1')\n",
    "lossesAdam_1_CNN = fit(modelAdam_1_CNN, train_dataloader, optimizerAdam_1_CNN, EPOCHS, device = DEVICE)\n",
    "predict(modelAdam_1_CNN, test_dataloader, device = DEVICE)\n",
    "plt.plot(lossesAdam_1_CNN,  label = \"Adam in CNN with LR = 1\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"Loss progression across epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scientific",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
